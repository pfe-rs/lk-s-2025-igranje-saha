{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA MANIPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chess\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  # For Jupyter Notebook\n",
    "import torch.nn.functional as F\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fen, move = self.data[idx]\n",
    "        \n",
    "        # Convert FEN to 8x8x20 tensor\n",
    "        board_tensor = torch.zeros((20, 8, 8), dtype=torch.float32)\n",
    "        board = chess.Board(fen)\n",
    "        \n",
    "        # Piece channels (0-11)\n",
    "        piece_map = {\n",
    "            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "        }\n",
    "        \n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                rank, file = 7 - square // 8, square % 8\n",
    "                board_tensor[piece_map[piece.symbol()], rank, file] = 1\n",
    "        \n",
    "        # Metadata channels (12-19)\n",
    "        board_tensor[12] = int(board.turn)  # White to move\n",
    "        board_tensor[13] = int(board.has_kingside_castling_rights(chess.WHITE))\n",
    "        board_tensor[14] = int(board.has_queenside_castling_rights(chess.WHITE))\n",
    "        board_tensor[15] = int(board.has_kingside_castling_rights(chess.BLACK))\n",
    "        board_tensor[16] = int(board.has_queenside_castling_rights(chess.BLACK))\n",
    "        board_tensor[17] = int(board.has_legal_en_passant())\n",
    "        board_tensor[18] = board.halfmove_clock / 50.0\n",
    "        board_tensor[19] = board.fullmove_number / 100.0\n",
    "        \n",
    "        # Convert move to label (simplified version)\n",
    "        move_label = self.move_to_index(move)\n",
    "        \n",
    "        return board_tensor, move_label\n",
    "    \n",
    "    def move_to_index(self, uci_move):\n",
    "        \"\"\"Convert UCI move to integer label (0-4671)\"\"\"\n",
    "        # Create all possible moves once and cache\n",
    "        if not hasattr(self, 'move_lookup'):\n",
    "            self.move_lookup = {}\n",
    "            label = 0\n",
    "            for from_sq in chess.SQUARES:\n",
    "                for to_sq in chess.SQUARES:\n",
    "                    if from_sq == to_sq:\n",
    "                        continue\n",
    "                    for promo in [None, 'q', 'r', 'b', 'n']:\n",
    "                        self.move_lookup[f\"{chess.square_name(from_sq)}{chess.square_name(to_sq)}{promo if promo else ''}\"] = label\n",
    "                        label += 1\n",
    "        return self.move_lookup.get(uci_move, 0)\n",
    "\n",
    "# Create datasets\n",
    "train_df = pd.read_csv(\"train_data.csv\", encoding = \"utf-8\")\n",
    "val_df = pd.read_csv(\"val_data.csv\", encoding = \"utf-8\")\n",
    "train_dataset = ChessDataset(train_df)\n",
    "val_dataset = ChessDataset(val_df)\n",
    "\n",
    "#print(train_dataset.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 40,982,208\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        return F.relu(x)\n",
    "\n",
    "class ChessResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(20, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(256),\n",
    "            ResBlock(256),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*8*8, 4672)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ChessResNet().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING - DON'T RUN IF TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893e8fb3c626493598111057d36815ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Train]:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m train_progress = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m'\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_progress:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     inputs, labels = \u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels.to(device)\n\u001b[32m     27\u001b[39m     optimizer.zero_grad()\n\u001b[32m     28\u001b[39m     outputs = model(inputs)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "# from tqdm import tqdm  # Use this if you're not in Jupyter\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Optimization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase with progress bar\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]', leave=False)\n",
    "    \n",
    "    for inputs, labels in train_progress:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_progress.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        })\n",
    "    \n",
    "    # Validation phase with progress bar\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_progress = tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_progress:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_progress.set_postfix({\n",
    "                'acc': f\"{correct/total:.2%}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "    history['val_acc'].append(epoch_acc)\n",
    "    \n",
    "    # Learning rate adjustment\n",
    "    scheduler.step(epoch_acc)\n",
    "    \n",
    "    # Epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} | Val Acc: {epoch_acc:.2%}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChessResNet()\n",
    "model.load_state_dict(torch.load('chess_resnet.pth', weights_only=True))\n",
    "#model = torch.load(model.state_dict(), 'chess_resnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12, 5))\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(history['train_loss'], label='Train Loss')\n",
    "#plt.title('Training Loss')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.plot(history['val_acc'], label='Val Accuracy')\n",
    "#plt.title('Validation Accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Save model\n",
    "#torch.save(model.state_dict(), 'chess_resnet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO PLAY WITH THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "class FenToTensor:\n",
    "    def __init__(self):\n",
    "        # Piece type mapping to channel indices\n",
    "        self.piece_map = {\n",
    "            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "        }\n",
    "    \n",
    "    def convert(self, fen):\n",
    "        \"\"\"Convert FEN string to 20x8x8 tensor\"\"\"\n",
    "        board_tensor = torch.zeros((20, 8, 8), dtype=torch.float32)\n",
    "        \n",
    "        # Parse FEN components\n",
    "        parts = fen.split()\n",
    "        board_part = parts[0]\n",
    "        active_color = parts[1]\n",
    "        castling = parts[2]\n",
    "        en_passant = parts[3]\n",
    "        halfmove_clock = int(parts[4])\n",
    "        fullmove_number = int(parts[5])\n",
    "        \n",
    "        # Process board position\n",
    "        self._process_board(board_part, board_tensor)\n",
    "        \n",
    "        # Process metadata\n",
    "        self._process_metadata(\n",
    "            board_tensor, active_color, castling, \n",
    "            en_passant, halfmove_clock, fullmove_number\n",
    "        )\n",
    "        \n",
    "        return board_tensor\n",
    "    \n",
    "    def _process_board(self, fen_board, tensor):\n",
    "        \"\"\"Process the board part of FEN into tensor channels 0-11\"\"\"\n",
    "        rank = 0  # 0 is top rank (a8-h8), 7 is bottom rank (a1-h1)\n",
    "        file = 0\n",
    "        \n",
    "        for char in fen_board:\n",
    "            if char == '/':\n",
    "                # Move to next rank\n",
    "                rank += 1\n",
    "                file = 0\n",
    "            elif char.isdigit():\n",
    "                # Skip empty squares\n",
    "                file += int(char)\n",
    "            else:\n",
    "                # Place the piece\n",
    "                channel = self.piece_map[char]\n",
    "                tensor[channel, rank, file] = 1\n",
    "                file += 1\n",
    "    \n",
    "    def _process_metadata(self, tensor, active_color, castling, \n",
    "                         en_passant, halfmove_clock, fullmove_number):\n",
    "        \"\"\"Process metadata into channels 12-19\"\"\"\n",
    "        # Channel 12: Turn (1 for white, 0 for black)\n",
    "        tensor[12] = 1 if active_color == 'w' else 0\n",
    "        \n",
    "        # Channels 13-16: Castling rights\n",
    "        tensor[13] = 1 if 'K' in castling else 0  # White kingside\n",
    "        tensor[14] = 1 if 'Q' in castling else 0  # White queenside\n",
    "        tensor[15] = 1 if 'k' in castling else 0  # Black kingside\n",
    "        tensor[16] = 1 if 'q' in castling else 0  # Black queenside\n",
    "        \n",
    "        # Channel 17: En passant target (1 if exists)\n",
    "        tensor[17] = 1 if en_passant != '-' else 0\n",
    "        \n",
    "        # Channel 18: Halfmove clock (normalized)\n",
    "        tensor[18] = min(halfmove_clock / 50.0, 1.0)\n",
    "        \n",
    "        # Channel 19: Fullmove number (normalized)\n",
    "        tensor[19] = min(fullmove_number / 100.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def index_to_uci(index: int) -> str:\n",
    "    \"\"\"Convert model's output index (0-4671) back to UCI move string.\"\"\"\n",
    "    # Generate all possible moves in the same order as ChessDataset\n",
    "    moves = []\n",
    "    for from_sq in chess.SQUARES:\n",
    "        for to_sq in chess.SQUARES:\n",
    "            if from_sq == to_sq:\n",
    "                continue  # Skip null moves\n",
    "            # Non-promotion case\n",
    "            moves.append(f\"{chess.square_name(from_sq)}{chess.square_name(to_sq)}\")\n",
    "            # Promotion cases (q, r, b, n)\n",
    "            for promo in ['q', 'r', 'b', 'n']:\n",
    "                moves.append(f\"{chess.square_name(from_sq)}{chess.square_name(to_sq)}{promo}\")\n",
    "    return moves[index] if index < len(moves) else \"0000\"\n",
    "\n",
    "def get_best_legal_move(model_output, fen):\n",
    "    board = chess.Board(fen)\n",
    "    if not board.legal_moves:\n",
    "        return None  # Game is over\n",
    "    \n",
    "    probs = torch.softmax(model_output, dim=1)[0]\n",
    "    legal_moves = []\n",
    "    \n",
    "    # Check all possible moves (not just top k)\n",
    "    for move_idx in range(len(probs)):\n",
    "        uci_move = index_to_uci(move_idx)\n",
    "        try:\n",
    "            move = chess.Move.from_uci(uci_move)\n",
    "            if move in board.legal_moves:\n",
    "                legal_moves.append((uci_move, probs[move_idx].item()))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not legal_moves:\n",
    "        print(\"RANDOM MOVE\")\n",
    "        return random.choice([move.uci() for move in board.legal_moves])\n",
    "    \n",
    "    # Return move with highest probability\n",
    "    legal_moves.sort(key=lambda x: x[1], reverse=True)\n",
    "    return legal_moves[0][0]\n",
    "\n",
    "converter = FenToTensor()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best legal move: None\n"
     ]
    }
   ],
   "source": [
    "TEMPERATURE = 0.75  # Try 0.5-1.0 for output sharpening\n",
    "fen = '6k1/1p3ppp/rr2p3/1Kpp1b2/p7/P3b1P1/4P1BP/8 w - - 7 34'\n",
    "\n",
    "tensor = converter.convert(fen)\n",
    "output = model(tensor.unsqueeze(0)) \n",
    "logits = output / TEMPERATURE\n",
    "\n",
    "output = torch.softmax(logits, dim=1)           # OK\n",
    "best_move = get_best_legal_move(output, fen)    # NE RADI \n",
    "\n",
    "print(f\"Best legal move: {best_move}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
